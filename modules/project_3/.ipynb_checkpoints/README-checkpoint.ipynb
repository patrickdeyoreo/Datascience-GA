{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![](https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png) Project 3: Feature Selection + Classification\n",
    "\n",
    "### Domain and Data\n",
    "\n",
    "This week, you've learned about access and utilizing remote databases, and more advanced topics for conducting logistic regression, selecting features, and building machine learning pipelines. Now, let's put these skills to the test!\n",
    "\n",
    "You're working as a data scientist with a research firm. You're firm is bidding on a big project that will involve working with thousands or possibly tens of thousands of features. You know it will be impossible to use conventional feature selection techniques. You propose that a way to win the contract is to demonstrate a capacity to identify relevant features using machine learning. Your boss says, \"Great idea. Write it up.\" You figure that working with a [Madelon](https://archive.ics.uci.edu/ml/datasets/Madelon)-style synthetic dataset is an excellent way to demonstrate your abilities. \n",
    "\n",
    "A data engineer colleague sets up a remote PostgreSQL database for you to work with upon which is loaded a massive dataset. You can connect to that database at `54.200.77.93:5432` to database `madelon` with user `postgres` and password \"`postgres`\". \n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "Your challenge here is to use machine learning techniques to identify important features and then use these techniques to do prediction on the entire set. \n",
    "\n",
    "### Solution Statement\n",
    "\n",
    "Your final product will consist of:\n",
    "\n",
    "1. A prepared report\n",
    "2. A series of Jupyter notebooks to be used to control your pipelines\n",
    "\n",
    "### Tasks\n",
    "\n",
    "#### Data Manipulation\n",
    "\n",
    "You should do substantive work on successively larger randomly selected subsets of the data. \n",
    "\n",
    "- Subset 1: .01% of the data\n",
    "- Subset 2: .1% of the data\n",
    "- Subset 3: 1% of the data\n",
    "- Subset 4: 10% of the data\n",
    "- Subset 5: 100% of the data\n",
    "\n",
    "##### Prepared Report\n",
    "\n",
    "Your report should:\n",
    "\n",
    "1. be a pdf\n",
    "2. present an analysis of data footprint of each subset and how it impacts your infrastructure\n",
    "   - e.g. `%whos`\n",
    "3. include EDA of each subset \n",
    "   - EDA needs may be different depending upon subset or your approach to a solution\n",
    "4. present results from Step 1: Benchmarking\n",
    "5. present results from Step 2: Identify Salient Features\n",
    "6. present results from Step 3: Feature Importances\n",
    "6. present results from Step 4: Build Model\n",
    "\n",
    "##### Jupyter Notebook, Data Footprint and EDA \n",
    "\n",
    "- study the size in memory of the data for each subset\n",
    "- present some sort of visualization of the data footprint\n",
    "- perform EDA on each set as you see necessary\n",
    "\n",
    "##### Jupyter Notebook, Step 1 - Benchmarking\n",
    "- build pipeline to perform a naive logistic regression as a baseline model\n",
    "- optionally, run one or more additional naive models e.g. decision tree, k nearest neighors, etc\n",
    "- use at least three different size subsets of the data\n",
    "- in order to do this, you will need to set a high `C` value in order to perform minimal regularization\n",
    "\n",
    "##### Jupyter Notebook, Step 2 - Identify Features\n",
    "- Build feature selection pipelines using at least three of these methods:\n",
    "   - a Lasso model\n",
    "   - SelectKBest\n",
    "   - SelectFromModel\n",
    "   - RFE\n",
    "   - PCA\n",
    "   - KernelPCA\n",
    "   - SVD\n",
    "- Perform each of the three on at least three subsets of data\n",
    "- **NOTE**: these pipelines are being used for feature selection not prediction\n",
    "\n",
    "##### Jupyter Notebook, Step 3 - Feature Importance\n",
    "- Use the results from step 2 to discuss feature importance in the dataset\n",
    "- Considering these results, develop a strategy for building a final predictive model\n",
    "- recommended approaches:\n",
    "    - Use feature selection to reduce the dataset to a manageable size then use conventional methods\n",
    "    - Use dimension reduction to reduce the dataset to a manageable size then use conventional methods\n",
    "    - Use an iterative model training method to use the entire dataset\n",
    "- Implement at least one of these on at least two different size subsets of data\n",
    "   \n",
    "##### Jupyter Notebook, Step 4 - Build Model\n",
    "- Implement your final model\n",
    "\n",
    "---\n",
    "\n",
    "### Requirements\n",
    "\n",
    "- Many Jupyter Notebooks\n",
    "- A written report of your findings that detail the accuracy and assumptions of your model.\n",
    "\n",
    "---\n",
    "\n",
    "### Suggestions\n",
    "\n",
    "- Document **everything**.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
